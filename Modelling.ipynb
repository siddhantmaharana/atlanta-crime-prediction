{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this module, we experiment with various machine learning techniques \n",
    "1. Base Estimators\n",
    "    a. Random Forest\n",
    "    b. KNN\n",
    "    c. QDA\n",
    "    d. SVC\n",
    "    e. Naive Baeyes\n",
    "2. Voting Algorithm\n",
    "3. Boosting \n",
    "    a. ADA Boost\n",
    "    b. Gradient Boosting\n",
    "    c. XG Boost\n",
    "    d. Hyper Parameter tuning\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Importing the libraries and the data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.svm import SVC\n",
    "from IPython.core.display import display, HTML\n",
    "import re as re\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Base Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initial Processing\n",
    "## Importing the dataset\n",
    "train = pd.read_csv('../data/train_with_features.csv')\n",
    "train = train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## \n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, LabelBinarizer\n",
    "features = ['num_victims', 'location_type', 'weekday', 'weekofyear',\\\n",
    "       'hourofday', 'workhour', 'sunlight', 'clustertype',\\\n",
    "       'crime_rate_LARCENY-FROM VEHICLE', 'crime_rate_LARCENY-NON VEHICLE',\\\n",
    "       'crime_rate_AUTO THEFT', 'crime_rate_ROBBERY-PEDESTRIAN',\\\n",
    "       'crime_rate_AGG ASSAULT', 'crime_rate_BURGLARY-RESIDENCE',\\\n",
    "       'crime_rate_BURGLARY-NONRES', 'crime_rate_ROBBERY-RESIDENCE',\\\n",
    "       'crime_rate_ROBBERY-COMMERCIAL', 'crime_rate_RAPE',\\\n",
    "        'crime_rate_HOMICIDE', 'count_crimes_lag_1',\\\n",
    "       'count_crimes_lag_7']\n",
    "\n",
    "\n",
    "## Importing features\n",
    "X = train[features].values\n",
    "Y = train['Crime_Type']\n",
    "## Encoding the target variable\n",
    "le = LabelEncoder()\n",
    "le.fit(Y)\n",
    "Y = le.transform(Y)\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "\n",
    "def test_classifier(classifier,X_train,X_test,y_train,y_test):\n",
    "    print (\"\")\n",
    "    print (\"===============================================\")\n",
    "    classifier_name = str(type(classifier).__name__)\n",
    "    print (\"Testing \" + classifier_name)\n",
    "\n",
    "    list_of_labels = sorted(list(set(y_train)))\n",
    "    model = classifier.fit(X_train, y_train)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, predictions,labels = list_of_labels)\n",
    "    pred_df = pd.crosstab(y_test,predictions)\n",
    "\n",
    "    recall = recall_score(y_test, predictions, average='weighted', pos_label=None, labels=list_of_labels)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print (\"=================== Results ===================\")\n",
    "\n",
    "    print (\"Recall: {0:.2f}%\".format(recall*100))\n",
    "    print (\"Accuracy: {0:.2f}%\".format(accuracy*100))\n",
    "    print (\"===============================================\")\n",
    "    return (pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1a.Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Runing random forest with default parameters\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_df = test_classifier(rf_clf,x_train,x_test,y_train,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tuning Random Forest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf_clf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# # Fit the random search model\n",
    "rf_random.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Runing random forest with tuned parameters \n",
    "rf_clf = RandomForestClassifier(n_estimators = 200, criterion = 'entropy', random_state = 0,max_depth=10)\n",
    "rf_df = test_classifier(rf_clf,x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1b.KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "k_range = range(30,50)\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, Y, cv=10, scoring='accuracy')\n",
    "    k_scores.append(scores.mean())\n",
    "print(\" results of KNN: \",k_scores)\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit K from above to get the base KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=33)\n",
    "knn_df = test_classifier(knn_clf,x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c. QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "qda_clf = QuadraticDiscriminantAnalysis()\n",
    "qda_df = test_classifier(qda_clf,x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d. SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "svc_clf = SVC(kernel=\"linear\", C=0.025)\n",
    "svc_df = test_classifier(svc_clf,x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1e Naive Baeyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf = GaussianNB()\n",
    "nb_df = test_classifier(nb_clf,x_train,x_test,y_train, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Voting Algorithm\n",
    "\n",
    "#####  These classifiers perform good on different target types. For example Random Forest and SVC tend to predict the majority classes well. On the other hand techniques like KNN , QDA perform well on the minor classes.  So we can do a voting to select the best predictions out of all these classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## checking Correlation for different Classifiers\n",
    "import seaborn as sns\n",
    "test_rf = pd.Series(rf_clf.predict(x_test),name ='RF')\n",
    "test_qda = pd.Series(qda_clf.predict(x_test),name ='QDA')\n",
    "test_knn = pd.Series(knn_clf.predict(x_test),name ='KNN')\n",
    "test_svc = pd.Series(svc_clf.predict(x_test),name ='SVC')\n",
    "test_nb = pd.Series(nb_clf.predict(x_test),name ='NB')\n",
    "\n",
    "ensemble_results = pd.concat([test_rf,test_qda,test_knn,test_svc,test_nb],axis=1)\n",
    "g= sns.heatmap(ensemble_results.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trying out voting using combinations of various classifiers\n",
    "votingC = VotingClassifier(estimators=[('RF', rf_clf),\n",
    "('QDA', qda_clf),('NB', nb_clf)], voting='soft', weights=[2,1,1], n_jobs=4)\n",
    "vote_df = test_classifier(votingC,x_train,x_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning the voting parameters\n",
    "\n",
    "import itertools as it   \n",
    "df_five = pd.DataFrame(columns=('w1', 'w2', 'w3','w4','w5', 'accuracy'))\n",
    "v_clfs = [('RF', rf_clf),('QDA', qda_clf),('NB', nb_clf),('SVC',svc_clf),('KNN',knn_clf)]\n",
    "# combs = list(it.combinations(v_clfs, 3))+ list(it.combinations(v_clfs, 4)) + \n",
    "combs_5 = list(it.combinations(v_clfs, 5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "            for w4 in range(1,4):\n",
    "                for w5 in range(1,4):\n",
    "                    for j,clf_comb in enumerate(combs_5):\n",
    "                        if len(set((w1,w2,w3,w4,w5))) == 1: # skip if all weights are equal\n",
    "                            continue   \n",
    "                        eclf = VotingClassifier(estimators=list(clf_comb),voting= 'soft', weights=[w1,w2,w3,w4,w5])\n",
    "                        model = eclf.fit(x_train, y_train)\n",
    "                        predictions = model.predict(x_test)\n",
    "                        accuracy = accuracy_score(y_test, predictions)\n",
    "                        print (w1,w2,w3,w4,w5,accuracy)\n",
    "                        df_five.loc[i] = [w1, w2, w3,w4,w5, accuracy]\n",
    "                        i += 1\n",
    "\n",
    "actual = pd.Series(y_test,name ='Actual')\n",
    "voting_results = pd.Series(votingC.predict(x_test),name ='Voting')\n",
    "ensemble_results = pd.concat([test_rf,test_qda,test_knn,test_svc,test_nb,actual,voting_results],axis=1)\n",
    "ensemble_results.to_csv('../data/voting_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Boosting Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. ADA Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed =7\n",
    "adaboost = AdaBoostClassifier()\n",
    "\n",
    "ada_df = test_classifier(adaboost,x_train,x_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = GradientBoostingClassifier(\n",
    "    max_depth=1,\n",
    "    n_estimators=500,\n",
    "    warm_start=True,\n",
    "    random_state=seed)\n",
    "\n",
    "gbc_df = test_classifier(gbc,x_train,x_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
